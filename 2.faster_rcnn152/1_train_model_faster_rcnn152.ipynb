{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e1cd147",
   "metadata": {},
   "source": [
    "# Tensorflow Object Detection API and AWS Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85592c17",
   "metadata": {},
   "source": [
    "In this notebook, you will train and evaluate different models using the [Tensorflow Object Detection API](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/) and [AWS Sagemaker](https://aws.amazon.com/sagemaker/). \n",
    "\n",
    "If you ever feel stuck, you can refer to this [tutorial](https://aws.amazon.com/blogs/machine-learning/training-and-deploying-models-using-tensorflow-2-with-the-object-detection-api-on-amazon-sagemaker/).\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We are using the [Waymo Open Dataset](https://waymo.com/open/) for this project. The dataset has already been exported using the tfrecords format. The files have been created following the format described [here](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html#create-tensorflow-records). You can find data stored on [AWS S3](https://aws.amazon.com/s3/), AWS Object Storage. The images are saved with a resolution of 640x640."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcc1d114",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "#%pip install tensorflow_io sagemaker -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96f55350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "from framework import CustomFramework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccde6fd1",
   "metadata": {},
   "source": [
    "Save the IAM role in a variable called `role`. This would be useful when training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ab6b13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "arn:aws:iam::029532071818:role/service-role/AmazonSageMaker-ExecutionRole-20231210T215923\n"
     ]
    }
   ],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae64e29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The train and val paths below are public S3 buckets created by Udacity for this project\n",
    "inputs = {'train': 's3://cd2688-object-detection-tf2/train/', \n",
    "          'val': 's3://cd2688-object-detection-tf2/val/'} \n",
    "\n",
    "# Insert path of a folder in your personal S3 bucket to store tensorboard logs.\n",
    "tensorboard_s3_prefix = 's3://minhnbbucket/logs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc16a825",
   "metadata": {},
   "source": [
    "## Container\n",
    "\n",
    "To train the model, you will first need to build a [docker](https://www.docker.com/) container with all the dependencies required by the TF Object Detection API. The code below does the following:\n",
    "* clone the Tensorflow models repository\n",
    "* get the exporter and training scripts from the repository\n",
    "* build the docker image and push it \n",
    "* print the container name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ad5ac8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# clone the repo and get the scripts\n",
    "#git clone https://github.com/tensorflow/models.git docker/models\n",
    "\n",
    "# get model_main and exporter_main files from TF2 Object Detection GitHub repository\n",
    "#cp docker/models/research/object_detection/exporter_main_v2.py source_dir \n",
    "#cp docker/models/research/object_detection/model_main_tf2.py source_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2dab3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build and push the docker image. This code can be commented out after being run once.\n",
    "# This will take around 10 mins.\n",
    "#image_name = 'tf2-object-detection'\n",
    "#!sh ./docker/build_and_push.sh $image_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62b3562",
   "metadata": {},
   "source": [
    "To verify that the image was correctly pushed to the [Elastic Container Registry](https://aws.amazon.com/ecr/), you can look at it in the AWS webapp. For example, below you can see that three different images have been pushed to ECR. You should only see one, called `tf2-object-detection`.\n",
    "![ECR Example](../data/example_ecr.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0310b6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "029532071818.dkr.ecr.us-east-1.amazonaws.com/tf2-object-detection:20231210132738\n"
     ]
    }
   ],
   "source": [
    "# display the container name\n",
    "with open (os.path.join('docker', 'ecr_image_fullname.txt'), 'r') as f:\n",
    "    container = f.readlines()[0][:-1]\n",
    "\n",
    "print(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b2a754",
   "metadata": {},
   "source": [
    "## Pre-trained model from model zoo\n",
    "\n",
    "As often, we are not training from scratch and we will be using a pretrained model from the TF Object Detection model zoo. You can find pretrained checkpoints [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md). Because your time is limited for this project, we recommend to only experiment with the following models:\n",
    "* SSD MobileNet V2 FPNLite 640x640\t\n",
    "* SSD ResNet50 V1 FPN 640x640 (RetinaNet50)\t\n",
    "* Faster R-CNN ResNet50 V1 640x640\t\n",
    "* EfficientDet D1 640x640\t\n",
    "* Faster R-CNN ResNet152 V1 640x640\t\n",
    "\n",
    "In the code below, the EfficientDet D1 model is downloaded and extracted. This code should be adjusted if you were to experiment with other architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c4b1d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#mkdir /tmp/faster_rcnn152_checkpoint\n",
    "#mkdir source_dir/faster_rcnn152_checkpoint\n",
    "#wget -O /tmp/faster_rcnn152.tar.gz http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_resnet152_v1_640x640_coco17_tpu-8.tar.gz\n",
    "#tar -zxvf /tmp/faster_rcnn152.tar.gz --strip-components 2 --directory source_dir/faster_rcnn152_checkpoint faster_rcnn_resnet152_v1_640x640_coco17_tpu-8/checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e04a98",
   "metadata": {},
   "source": [
    "## Edit pipeline.config file\n",
    "\n",
    "The [`pipeline.config`](source_dir/pipeline.config) in the `source_dir` folder should be updated when you experiment with different models. The different config files are available [here](https://github.com/tensorflow/models/tree/master/research/object_detection/configs/tf2).\n",
    "\n",
    ">Note: The provided `pipeline.config` file works well with the `EfficientDet` model. You would need to modify it when working with other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47483545",
   "metadata": {},
   "source": [
    "## Launch Training Job\n",
    "\n",
    "Now that we have a dataset, a docker image and some pretrained model weights, we can launch the training job. To do so, we create a [Sagemaker Framework](https://sagemaker.readthedocs.io/en/stable/frameworks/index.html), where we indicate the container name, name of the config file, number of training steps etc.\n",
    "\n",
    "The `run_training.sh` script does the following:\n",
    "* train the model for `num_train_steps` \n",
    "* evaluate over the val dataset\n",
    "* export the model\n",
    "\n",
    "Different metrics will be displayed during the evaluation phase, including the mean average precision. These metrics can be used to quantify your model performances and compare over the different iterations.\n",
    "\n",
    "You can also monitor the training progress by navigating to **Training -> Training Jobs** from the Amazon Sagemaker dashboard in the Web UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c7175cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: faster-rcnn152-tf2-object-detection-2023-12-13-15-32-00-837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-13 15:32:21 Starting - Starting the training job...\n",
      "2023-12-13 15:32:37 Starting - Preparing the instances for training.........\n",
      "2023-12-13 15:34:11 Downloading - Downloading input data...\n",
      "2023-12-13 15:34:40 Downloading - Downloading the training image............\n",
      "2023-12-13 15:36:41 Training - Training image download completed. Training in progress.....\u001b[34m2023-12-13 15:37:15,360 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-12-13 15:37:15,393 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-12-13 15:37:15,426 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-12-13 15:37:15,438 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"val\": \"/opt/ml/input/data/val\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": null,\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"model_dir\": \"/opt/training\",\n",
      "        \"num_train_steps\": \"1000\",\n",
      "        \"pipeline_config_path\": \"Faster_R_CNN_ResNet152_pipeline.config\",\n",
      "        \"sample_1_of_n_eval_examples\": \"1\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"val\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": false,\n",
      "    \"job_name\": \"faster-rcnn152-tf2-object-detection-2023-12-13-15-32-00-837\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-029532071818/faster-rcnn152-tf2-object-detection-2023-12-13-15-32-00-837/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_training.sh\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_training.sh\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"model_dir\":\"/opt/training\",\"num_train_steps\":\"1000\",\"pipeline_config_path\":\"Faster_R_CNN_ResNet152_pipeline.config\",\"sample_1_of_n_eval_examples\":\"1\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_training.sh\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"val\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_training.sh\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-029532071818/faster-rcnn152-tf2-object-detection-2023-12-13-15-32-00-837/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"val\":\"/opt/ml/input/data/val\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":null,\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"model_dir\":\"/opt/training\",\"num_train_steps\":\"1000\",\"pipeline_config_path\":\"Faster_R_CNN_ResNet152_pipeline.config\",\"sample_1_of_n_eval_examples\":\"1\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":false,\"job_name\":\"faster-rcnn152-tf2-object-detection-2023-12-13-15-32-00-837\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-029532071818/faster-rcnn152-tf2-object-detection-2023-12-13-15-32-00-837/source/sourcedir.tar.gz\",\"module_name\":\"run_training.sh\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_training.sh\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--model_dir\",\"/opt/training\",\"--num_train_steps\",\"1000\",\"--pipeline_config_path\",\"Faster_R_CNN_ResNet152_pipeline.config\",\"--sample_1_of_n_eval_examples\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VAL=/opt/ml/input/data/val\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=/opt/training\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_STEPS=1000\u001b[0m\n",
      "\u001b[34mSM_HP_PIPELINE_CONFIG_PATH=Faster_R_CNN_ResNet152_pipeline.config\u001b[0m\n",
      "\u001b[34mSM_HP_SAMPLE_1_OF_N_EVAL_EXAMPLES=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python38.zip:/usr/lib/python3.8:/usr/lib/python3.8/lib-dynload:/usr/local/lib/python3.8/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/bin/sh -c \"./run_training.sh --model_dir /opt/training --num_train_steps 1000 --pipeline_config_path Faster_R_CNN_ResNet152_pipeline.config --sample_1_of_n_eval_examples 1\"\u001b[0m\n",
      "\u001b[34m2023-12-13 15:37:15,438 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m===TRAINING THE MODEL==\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\u001b[0m\n",
      "\u001b[34mI1213 15:37:21.019327 140241462662976 mirrored_strategy.py:419] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Maybe overwriting train_steps: 1000\u001b[0m\n",
      "\u001b[34mI1213 15:37:21.210879 140241462662976 config_util.py:552] Maybe overwriting train_steps: 1000\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Maybe overwriting use_bfloat16: False\u001b[0m\n",
      "\u001b[34mI1213 15:37:21.211031 140241462662976 config_util.py:552] Maybe overwriting use_bfloat16: False\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mrename to distribute_datasets_from_function\u001b[0m\n",
      "\u001b[34mW1213 15:37:21.241354 140241462662976 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mrename to distribute_datasets_from_function\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Reading unweighted datasets: ['/opt/ml/input/data/train/*.tfrecord']\u001b[0m\n",
      "\u001b[34mI1213 15:37:21.247147 140241462662976 dataset_builder.py:162] Reading unweighted datasets: ['/opt/ml/input/data/train/*.tfrecord']\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Reading record datasets for input file: ['/opt/ml/input/data/train/*.tfrecord']\u001b[0m\n",
      "\u001b[34mI1213 15:37:21.248937 140241462662976 dataset_builder.py:79] Reading record datasets for input file: ['/opt/ml/input/data/train/*.tfrecord']\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Number of filenames to read: 84\u001b[0m\n",
      "\u001b[34mI1213 15:37:21.249030 140241462662976 dataset_builder.py:80] Number of filenames to read: 84\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\u001b[0m\n",
      "\u001b[34mW1213 15:37:21.255061 140241462662976 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.data.Dataset.map()\u001b[0m\n",
      "\u001b[34mW1213 15:37:21.270243 140241462662976 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.data.Dataset.map()\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mCreate a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\u001b[0m\n",
      "\u001b[34mW1213 15:37:27.456020 140241462662976 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mCreate a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.cast` instead.\u001b[0m\n",
      "\u001b[34mW1213 15:37:30.053304 140241462662976 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.cast` instead.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/keras/src/backend.py:452: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:depth of additional conv before box predictor: 0\u001b[0m\n",
      "\u001b[34mI1213 15:37:42.676768 140211916879616 convolutional_keras_box_predictor.py:152] depth of additional conv before box predictor: 0\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:460: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse ref() instead.\u001b[0m\n",
      "\u001b[34mW1213 15:37:52.270421 140211916879616 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:460: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse ref() instead.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mFuture major versions of TensorFlow will allow gradients to flow\u001b[0m\n",
      "\u001b[34minto the labels input on backprop by default.\u001b[0m\n",
      "\u001b[34mSee `tf.nn.softmax_cross_entropy_with_logits_v2`.\u001b[0m\n",
      "\u001b[34mW1213 15:37:57.788561 140211916879616 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mFuture major versions of TensorFlow will allow gradients to flow\u001b[0m\n",
      "\u001b[34minto the labels input on backprop by default.\u001b[0m\n",
      "\u001b[34mSee `tf.nn.softmax_cross_entropy_with_logits_v2`.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mI1213 15:38:22.953470 140241462662976 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mI1213 15:38:22.955663 140241462662976 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mI1213 15:38:22.956407 140241462662976 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mI1213 15:38:22.957025 140241462662976 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mI1213 15:38:22.959654 140241462662976 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mI1213 15:38:22.960350 140241462662976 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mI1213 15:38:22.961111 140241462662976 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mI1213 15:38:22.961818 140241462662976 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mI1213 15:38:22.964533 140241462662976 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mI1213 15:38:22.965197 140241462662976 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/deprecation.py:648: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse fn_output_signature instead\u001b[0m\n",
      "\u001b[34mW1213 15:38:24.604835 140211199661824 deprecation.py:569] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/deprecation.py:648: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse fn_output_signature instead\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 100 per-step time 2.100s\u001b[0m\n",
      "\u001b[34mI1213 15:41:54.380466 140241462662976 model_lib_v2.py:705] Step 100 per-step time 2.100s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/BoxClassifierLoss/classification_loss': 0.17386237,\n",
      " 'Loss/BoxClassifierLoss/localization_loss': 0.30186686,\n",
      " 'Loss/RPNLoss/localization_loss': 0.29753032,\n",
      " 'Loss/RPNLoss/objectness_loss': 0.06579864,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 0.83905816,\n",
      " 'learning_rate': 0.014666351}\u001b[0m\n",
      "\u001b[34mI1213 15:41:54.380755 140241462662976 model_lib_v2.py:708] {'Loss/BoxClassifierLoss/classification_loss': 0.17386237,\n",
      " 'Loss/BoxClassifierLoss/localization_loss': 0.30186686,\n",
      " 'Loss/RPNLoss/localization_loss': 0.29753032,\n",
      " 'Loss/RPNLoss/objectness_loss': 0.06579864,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 0.83905816,\n",
      " 'learning_rate': 0.014666351}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 200 per-step time 1.143s\u001b[0m\n",
      "\u001b[34mI1213 15:43:48.557384 140241462662976 model_lib_v2.py:705] Step 200 per-step time 1.143s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/BoxClassifierLoss/classification_loss': 0.16410452,\n",
      " 'Loss/BoxClassifierLoss/localization_loss': 0.33631632,\n",
      " 'Loss/RPNLoss/localization_loss': 0.25697085,\n",
      " 'Loss/RPNLoss/objectness_loss': 0.031009704,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 0.7884014,\n",
      " 'learning_rate': 0.0159997}\u001b[0m\n",
      "\u001b[34mI1213 15:43:48.557628 140241462662976 model_lib_v2.py:708] {'Loss/BoxClassifierLoss/classification_loss': 0.16410452,\n",
      " 'Loss/BoxClassifierLoss/localization_loss': 0.33631632,\n",
      " 'Loss/RPNLoss/localization_loss': 0.25697085,\n",
      " 'Loss/RPNLoss/objectness_loss': 0.031009704,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 0.7884014,\n",
      " 'learning_rate': 0.0159997}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 300 per-step time 1.137s\u001b[0m\n",
      "\u001b[34mI1213 15:45:42.284142 140241462662976 model_lib_v2.py:705] Step 300 per-step time 1.137s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/BoxClassifierLoss/classification_loss': 0.27261746,\n",
      " 'Loss/BoxClassifierLoss/localization_loss': 0.4272223,\n",
      " 'Loss/RPNLoss/localization_loss': 0.5007895,\n",
      " 'Loss/RPNLoss/objectness_loss': 0.03406182,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 1.2346911,\n",
      " 'learning_rate': 0.01733305}\u001b[0m\n",
      "\u001b[34mI1213 15:45:42.284436 140241462662976 model_lib_v2.py:708] {'Loss/BoxClassifierLoss/classification_loss': 0.27261746,\n",
      " 'Loss/BoxClassifierLoss/localization_loss': 0.4272223,\n",
      " 'Loss/RPNLoss/localization_loss': 0.5007895,\n",
      " 'Loss/RPNLoss/objectness_loss': 0.03406182,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 1.2346911,\n",
      " 'learning_rate': 0.01733305}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO:tensorflow:Step 400 per-step time 1.140s\u001b[0m\n",
      "\u001b[34mI1213 15:47:36.279885 140241462662976 model_lib_v2.py:705] Step 400 per-step time 1.140s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/BoxClassifierLoss/classification_loss': 0.12944381,\n",
      " 'Loss/BoxClassifierLoss/localization_loss': 0.32092515,\n",
      " 'Loss/RPNLoss/localization_loss': 0.20453805,\n",
      " 'Loss/RPNLoss/objectness_loss': 0.01858524,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 0.6734923,\n",
      " 'learning_rate': 0.0186664}\u001b[0m\n",
      "\u001b[34mI1213 15:47:36.280187 140241462662976 model_lib_v2.py:708] {'Loss/BoxClassifierLoss/classification_loss': 0.12944381,\n",
      " 'Loss/BoxClassifierLoss/localization_loss': 0.32092515,\n",
      " 'Loss/RPNLoss/localization_loss': 0.20453805,\n",
      " 'Loss/RPNLoss/objectness_loss': 0.01858524,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 0.6734923,\n",
      " 'learning_rate': 0.0186664}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 500 per-step time 1.140s\u001b[0m\n",
      "\u001b[34mI1213 15:49:30.306316 140241462662976 model_lib_v2.py:705] Step 500 per-step time 1.140s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/BoxClassifierLoss/classification_loss': 0.16666251,\n",
      " 'Loss/BoxClassifierLoss/localization_loss': 0.25280726,\n",
      " 'Loss/RPNLoss/localization_loss': 0.39252153,\n",
      " 'Loss/RPNLoss/objectness_loss': 0.028700136,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 0.8406914,\n",
      " 'learning_rate': 0.01999975}\u001b[0m\n",
      "\u001b[34mI1213 15:49:30.306566 140241462662976 model_lib_v2.py:708] {'Loss/BoxClassifierLoss/classification_loss': 0.16666251,\n",
      " 'Loss/BoxClassifierLoss/localization_loss': 0.25280726,\n",
      " 'Loss/RPNLoss/localization_loss': 0.39252153,\n",
      " 'Loss/RPNLoss/objectness_loss': 0.028700136,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 0.8406914,\n",
      " 'learning_rate': 0.01999975}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 600 per-step time 1.139s\u001b[0m\n",
      "\u001b[34mI1213 15:51:24.254518 140241462662976 model_lib_v2.py:705] Step 600 per-step time 1.139s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/BoxClassifierLoss/classification_loss': 0.10087439,\n",
      " 'Loss/BoxClassifierLoss/localization_loss': 0.16498107,\n",
      " 'Loss/RPNLoss/localization_loss': 0.17179947,\n",
      " 'Loss/RPNLoss/objectness_loss': 0.01197301,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 0.44962797,\n",
      " 'learning_rate': 0.0213331}\u001b[0m\n",
      "\u001b[34mI1213 15:51:24.254760 140241462662976 model_lib_v2.py:708] {'Loss/BoxClassifierLoss/classification_loss': 0.10087439,\n",
      " 'Loss/BoxClassifierLoss/localization_loss': 0.16498107,\n",
      " 'Loss/RPNLoss/localization_loss': 0.17179947,\n",
      " 'Loss/RPNLoss/objectness_loss': 0.01197301,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 0.44962797,\n",
      " 'learning_rate': 0.0213331}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 700 per-step time 1.138s\u001b[0m\n",
      "\u001b[34mI1213 15:53:18.067627 140241462662976 model_lib_v2.py:705] Step 700 per-step time 1.138s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/BoxClassifierLoss/classification_loss': 0.17839572,\n",
      " 'Loss/BoxClassifierLoss/localization_loss': 0.2705202,\n",
      " 'Loss/RPNLoss/localization_loss': 0.33277526,\n",
      " 'Loss/RPNLoss/objectness_loss': 0.020566676,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 0.8022579,\n",
      " 'learning_rate': 0.02266645}\u001b[0m\n",
      "\u001b[34mI1213 15:53:18.067882 140241462662976 model_lib_v2.py:708] {'Loss/BoxClassifierLoss/classification_loss': 0.17839572,\n",
      " 'Loss/BoxClassifierLoss/localization_loss': 0.2705202,\n",
      " 'Loss/RPNLoss/localization_loss': 0.33277526,\n",
      " 'Loss/RPNLoss/objectness_loss': 0.020566676,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 0.8022579,\n",
      " 'learning_rate': 0.02266645}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 800 per-step time 1.140s\u001b[0m\n",
      "\u001b[34mI1213 15:55:12.081682 140241462662976 model_lib_v2.py:705] Step 800 per-step time 1.140s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/BoxClassifierLoss/classification_loss': 0.22173001,\n",
      " 'Loss/BoxClassifierLoss/localization_loss': 0.4257691,\n",
      " 'Loss/RPNLoss/localization_loss': 0.31371668,\n",
      " 'Loss/RPNLoss/objectness_loss': 0.027946798,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 0.9891625,\n",
      " 'learning_rate': 0.023999799}\u001b[0m\n",
      "\u001b[34mI1213 15:55:12.081938 140241462662976 model_lib_v2.py:708] {'Loss/BoxClassifierLoss/classification_loss': 0.22173001,\n",
      " 'Loss/BoxClassifierLoss/localization_loss': 0.4257691,\n",
      " 'Loss/RPNLoss/localization_loss': 0.31371668,\n",
      " 'Loss/RPNLoss/objectness_loss': 0.027946798,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 0.9891625,\n",
      " 'learning_rate': 0.023999799}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 900 per-step time 1.140s\u001b[0m\n",
      "\u001b[34mI1213 15:57:06.057399 140241462662976 model_lib_v2.py:705] Step 900 per-step time 1.140s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/BoxClassifierLoss/classification_loss': 0.2389234,\n",
      " 'Loss/BoxClassifierLoss/localization_loss': 0.30679038,\n",
      " 'Loss/RPNLoss/localization_loss': 0.2490749,\n",
      " 'Loss/RPNLoss/objectness_loss': 0.067249745,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 0.8620385,\n",
      " 'learning_rate': 0.025333151}\u001b[0m\n",
      "\u001b[34mI1213 15:57:06.057663 140241462662976 model_lib_v2.py:708] {'Loss/BoxClassifierLoss/classification_loss': 0.2389234,\n",
      " 'Loss/BoxClassifierLoss/localization_loss': 0.30679038,\n",
      " 'Loss/RPNLoss/localization_loss': 0.2490749,\n",
      " 'Loss/RPNLoss/objectness_loss': 0.067249745,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 0.8620385,\n",
      " 'learning_rate': 0.025333151}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 1000 per-step time 1.139s\u001b[0m\n",
      "\u001b[34mI1213 15:58:59.986063 140241462662976 model_lib_v2.py:705] Step 1000 per-step time 1.139s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/BoxClassifierLoss/classification_loss': 0.21628007,\n",
      " 'Loss/BoxClassifierLoss/localization_loss': 0.36454335,\n",
      " 'Loss/RPNLoss/localization_loss': 0.23533309,\n",
      " 'Loss/RPNLoss/objectness_loss': 0.014613239,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 0.83076966,\n",
      " 'learning_rate': 0.0266665}\u001b[0m\n",
      "\u001b[34mI1213 15:58:59.986314 140241462662976 model_lib_v2.py:708] {'Loss/BoxClassifierLoss/classification_loss': 0.21628007,\n",
      " 'Loss/BoxClassifierLoss/localization_loss': 0.36454335,\n",
      " 'Loss/RPNLoss/localization_loss': 0.23533309,\n",
      " 'Loss/RPNLoss/objectness_loss': 0.014613239,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 0.83076966,\n",
      " 'learning_rate': 0.0266665}\u001b[0m\n",
      "\u001b[34m==EVALUATING THE MODEL==\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Forced number of epochs for all eval validations to be 1.\u001b[0m\n",
      "\u001b[34mW1213 15:59:10.227138 140467881867072 model_lib_v2.py:1089] Forced number of epochs for all eval validations to be 1.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Maybe overwriting sample_1_of_n_eval_examples: None\u001b[0m\n",
      "\u001b[34mI1213 15:59:10.227498 140467881867072 config_util.py:552] Maybe overwriting sample_1_of_n_eval_examples: None\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Maybe overwriting use_bfloat16: False\u001b[0m\n",
      "\u001b[34mI1213 15:59:10.227606 140467881867072 config_util.py:552] Maybe overwriting use_bfloat16: False\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Maybe overwriting eval_num_epochs: 1\u001b[0m\n",
      "\u001b[34mI1213 15:59:10.227728 140467881867072 config_util.py:552] Maybe overwriting eval_num_epochs: 1\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\u001b[0m\n",
      "\u001b[34mW1213 15:59:10.227864 140467881867072 model_lib_v2.py:1106] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Reading unweighted datasets: ['/opt/ml/input/data/val/*.tfrecord']\u001b[0m\n",
      "\u001b[34mI1213 15:59:10.576318 140467881867072 dataset_builder.py:162] Reading unweighted datasets: ['/opt/ml/input/data/val/*.tfrecord']\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Reading record datasets for input file: ['/opt/ml/input/data/val/*.tfrecord']\u001b[0m\n",
      "\u001b[34mI1213 15:59:10.577473 140467881867072 dataset_builder.py:79] Reading record datasets for input file: ['/opt/ml/input/data/val/*.tfrecord']\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Number of filenames to read: 13\u001b[0m\n",
      "\u001b[34mI1213 15:59:10.577555 140467881867072 dataset_builder.py:80] Number of filenames to read: 13\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:num_readers has been reduced to 13 to match input file shards.\u001b[0m\n",
      "\u001b[34mW1213 15:59:10.577647 140467881867072 dataset_builder.py:86] num_readers has been reduced to 13 to match input file shards.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:`shuffle` is false, but the input data stream is still slightly shuffled since `num_readers` > 1.\u001b[0m\n",
      "\u001b[34mW1213 15:59:10.579733 140467881867072 dataset_builder.py:93] `shuffle` is false, but the input data stream is still slightly shuffled since `num_readers` > 1.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\u001b[0m\n",
      "\u001b[34mW1213 15:59:10.581091 140467881867072 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.data.Dataset.map()\u001b[0m\n",
      "\u001b[34mW1213 15:59:10.597076 140467881867072 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.data.Dataset.map()\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mCreate a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\u001b[0m\n",
      "\u001b[34mW1213 15:59:14.286785 140467881867072 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mCreate a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.cast` instead.\u001b[0m\n",
      "\u001b[34mW1213 15:59:15.545883 140467881867072 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.cast` instead.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Waiting for new checkpoint at /opt/training\u001b[0m\n",
      "\u001b[34mI1213 15:59:17.988697 140467881867072 checkpoint_utils.py:168] Waiting for new checkpoint at /opt/training\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Found new checkpoint at /opt/training/ckpt-2\u001b[0m\n",
      "\u001b[34mI1213 15:59:17.989280 140467881867072 checkpoint_utils.py:177] Found new checkpoint at /opt/training/ckpt-2\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/keras/src/backend.py:452: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:depth of additional conv before box predictor: 0\u001b[0m\n",
      "\u001b[34mI1213 15:59:28.052126 140467881867072 convolutional_keras_box_predictor.py:152] depth of additional conv before box predictor: 0\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:460: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse ref() instead.\u001b[0m\n",
      "\u001b[34mW1213 15:59:34.271592 140467881867072 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:460: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse ref() instead.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mFuture major versions of TensorFlow will allow gradients to flow\u001b[0m\n",
      "\u001b[34minto the labels input on backprop by default.\u001b[0m\n",
      "\u001b[34mSee `tf.nn.softmax_cross_entropy_with_logits_v2`.\u001b[0m\n",
      "\u001b[34mW1213 15:59:38.415650 140467881867072 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mFuture major versions of TensorFlow will allow gradients to flow\u001b[0m\n",
      "\u001b[34minto the labels input on backprop by default.\u001b[0m\n",
      "\u001b[34mSee `tf.nn.softmax_cross_entropy_with_logits_v2`.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.cast` instead.\u001b[0m\n",
      "\u001b[34mW1213 15:59:48.258745 140467881867072 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.cast` instead.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Finished eval step 0\u001b[0m\n",
      "\u001b[34mI1213 15:59:48.314358 140467881867072 model_lib_v2.py:966] Finished eval step 0\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:460: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mtf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \u001b[0m\n",
      "\u001b[34mW1213 15:59:48.445755 140467881867072 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:460: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mtf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Finished eval step 100\u001b[0m\n",
      "\u001b[34mI1213 15:59:57.947090 140467881867072 model_lib_v2.py:966] Finished eval step 100\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Finished eval step 200\u001b[0m\n",
      "\u001b[34mI1213 16:00:04.980517 140467881867072 model_lib_v2.py:966] Finished eval step 200\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Performing evaluation on 258 images.\u001b[0m\n",
      "\u001b[34mI1213 16:00:08.867334 140467881867072 coco_evaluation.py:293] Performing evaluation on 258 images.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Loading and preparing annotation results...\u001b[0m\n",
      "\u001b[34mI1213 16:00:08.872148 140467881867072 coco_tools.py:116] Loading and preparing annotation results...\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:DONE (t=0.03s)\u001b[0m\n",
      "\u001b[34mI1213 16:00:08.897619 140467881867072 coco_tools.py:138] DONE (t=0.03s)\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Eval metrics at step 1000\u001b[0m\n",
      "\u001b[34mI1213 16:00:22.119842 140467881867072 model_lib_v2.py:1015] Eval metrics at step 1000\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Precision/mAP: 0.114752\u001b[0m\n",
      "\u001b[34mI1213 16:00:22.162810 140467881867072 model_lib_v2.py:1018] #011+ DetectionBoxes_Precision/mAP: 0.114752\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Precision/mAP@.50IOU: 0.241791\u001b[0m\n",
      "\u001b[34mI1213 16:00:22.164201 140467881867072 model_lib_v2.py:1018] #011+ DetectionBoxes_Precision/mAP@.50IOU: 0.241791\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Precision/mAP@.75IOU: 0.094256\u001b[0m\n",
      "\u001b[34mI1213 16:00:22.165120 140467881867072 model_lib_v2.py:1018] #011+ DetectionBoxes_Precision/mAP@.75IOU: 0.094256\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Precision/mAP (small): 0.050127\u001b[0m\n",
      "\u001b[34mI1213 16:00:22.166108 140467881867072 model_lib_v2.py:1018] #011+ DetectionBoxes_Precision/mAP (small): 0.050127\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Precision/mAP (medium): 0.383969\u001b[0m\n",
      "\u001b[34mI1213 16:00:22.167080 140467881867072 model_lib_v2.py:1018] #011+ DetectionBoxes_Precision/mAP (medium): 0.383969\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Precision/mAP (large): 0.743631\u001b[0m\n",
      "\u001b[34mI1213 16:00:22.168023 140467881867072 model_lib_v2.py:1018] #011+ DetectionBoxes_Precision/mAP (large): 0.743631\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Recall/AR@1: 0.028800\u001b[0m\n",
      "\u001b[34mI1213 16:00:22.168994 140467881867072 model_lib_v2.py:1018] #011+ DetectionBoxes_Recall/AR@1: 0.028800\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Recall/AR@10: 0.119797\u001b[0m\n",
      "\u001b[34mI1213 16:00:22.169939 140467881867072 model_lib_v2.py:1018] #011+ DetectionBoxes_Recall/AR@10: 0.119797\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Recall/AR@100: 0.157117\u001b[0m\n",
      "\u001b[34mI1213 16:00:22.170840 140467881867072 model_lib_v2.py:1018] #011+ DetectionBoxes_Recall/AR@100: 0.157117\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Recall/AR@100 (small): 0.089364\u001b[0m\n",
      "\u001b[34mI1213 16:00:22.171769 140467881867072 model_lib_v2.py:1018] #011+ DetectionBoxes_Recall/AR@100 (small): 0.089364\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Recall/AR@100 (medium): 0.493113\u001b[0m\n",
      "\u001b[34mI1213 16:00:22.172703 140467881867072 model_lib_v2.py:1018] #011+ DetectionBoxes_Recall/AR@100 (medium): 0.493113\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Recall/AR@100 (large): 0.759316\u001b[0m\n",
      "\u001b[34mI1213 16:00:22.173678 140467881867072 model_lib_v2.py:1018] #011+ DetectionBoxes_Recall/AR@100 (large): 0.759316\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ Loss/RPNLoss/localization_loss: 0.537971\u001b[0m\n",
      "\u001b[34mI1213 16:00:22.174398 140467881867072 model_lib_v2.py:1018] #011+ Loss/RPNLoss/localization_loss: 0.537971\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ Loss/RPNLoss/objectness_loss: 0.050380\u001b[0m\n",
      "\u001b[34mI1213 16:00:22.175132 140467881867072 model_lib_v2.py:1018] #011+ Loss/RPNLoss/objectness_loss: 0.050380\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ Loss/BoxClassifierLoss/localization_loss: 0.242681\u001b[0m\n",
      "\u001b[34mI1213 16:00:22.175866 140467881867072 model_lib_v2.py:1018] #011+ Loss/BoxClassifierLoss/localization_loss: 0.242681\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ Loss/BoxClassifierLoss/classification_loss: 0.188763\u001b[0m\n",
      "\u001b[34mI1213 16:00:22.176634 140467881867072 model_lib_v2.py:1018] #011+ Loss/BoxClassifierLoss/classification_loss: 0.188763\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ Loss/regularization_loss: 0.000000\u001b[0m\n",
      "\u001b[34mI1213 16:00:22.177413 140467881867072 model_lib_v2.py:1018] #011+ Loss/regularization_loss: 0.000000\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ Loss/total_loss: 1.019795\u001b[0m\n",
      "\u001b[34mI1213 16:00:22.178144 140467881867072 model_lib_v2.py:1018] #011+ Loss/total_loss: 1.019795\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO:tensorflow:Waiting for new checkpoint at /opt/training\u001b[0m\n",
      "\u001b[34mI1213 16:04:18.089302 140467881867072 checkpoint_utils.py:168] Waiting for new checkpoint at /opt/training\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Timed-out waiting for a checkpoint.\u001b[0m\n",
      "\u001b[34mI1213 16:04:27.101689 140467881867072 checkpoint_utils.py:231] Timed-out waiting for a checkpoint.\u001b[0m\n",
      "\u001b[34mcreating index...\u001b[0m\n",
      "\u001b[34mindex created!\u001b[0m\n",
      "\u001b[34mcreating index...\u001b[0m\n",
      "\u001b[34mindex created!\u001b[0m\n",
      "\u001b[34mRunning per image evaluation...\u001b[0m\n",
      "\u001b[34mEvaluate annotation type *bbox*\u001b[0m\n",
      "\u001b[34mDONE (t=12.82s).\u001b[0m\n",
      "\u001b[34mAccumulating evaluation results...\u001b[0m\n",
      "\u001b[34mDONE (t=0.35s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.115\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.242\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.094\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.050\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.384\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.744\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.029\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.120\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.157\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.089\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.493\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.759\u001b[0m\n",
      "\u001b[34m==EXPORTING THE MODEL==\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:459: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mback_prop=False is deprecated. Consider using tf.stop_gradient instead.\u001b[0m\n",
      "\u001b[34mInstead of:\u001b[0m\n",
      "\u001b[34mresults = tf.map_fn(fn, elems, back_prop=False)\u001b[0m\n",
      "\u001b[34mUse:\u001b[0m\n",
      "\u001b[34mresults = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\u001b[0m\n",
      "\u001b[34mW1213 16:04:31.377828 139935007315776 deprecation.py:641] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:459: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mback_prop=False is deprecated. Consider using tf.stop_gradient instead.\u001b[0m\n",
      "\u001b[34mInstead of:\u001b[0m\n",
      "\u001b[34mresults = tf.map_fn(fn, elems, back_prop=False)\u001b[0m\n",
      "\u001b[34mUse:\u001b[0m\n",
      "\u001b[34mresults = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:depth of additional conv before box predictor: 0\u001b[0m\n",
      "\u001b[34mI1213 16:04:40.293573 139935007315776 convolutional_keras_box_predictor.py:152] depth of additional conv before box predictor: 0\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:460: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse ref() instead.\u001b[0m\n",
      "\u001b[34mW1213 16:04:46.929972 139935007315776 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:460: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse ref() instead.\u001b[0m\n",
      "\u001b[34mI1213 16:04:53.609322 139935007315776 signature_serialization.py:148] Function `call_func` contains input name(s) resource with unsupported characters which will be renamed to mask_rcnn_keras_box_predictor_mask_rcnn_class_head_classpredictor_dense_biasadd_readvariableop_resource in the SavedModel.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.meta_architectures.faster_rcnn_meta_arch.FasterRCNNMetaArch object at 0x7f444ebe12e0>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1213 16:04:58.012211 139935007315776 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.meta_architectures.faster_rcnn_meta_arch.FasterRCNNMetaArch object at 0x7f444ebe12e0>, because it is not built.\u001b[0m\n",
      "\u001b[34mI1213 16:05:25.958154 139935007315776 save.py:274] Found untraced functions such as FirstStageBoxPredictor_layer_call_fn, FirstStageBoxPredictor_layer_call_and_return_conditional_losses, mask_rcnn_keras_box_predictor_layer_call_fn, mask_rcnn_keras_box_predictor_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op while saving (showing 5 of 186). These functions will not be directly callable after loading.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Assets written to: /tmp/exported/saved_model/assets\u001b[0m\n",
      "\u001b[34mI1213 16:05:37.721501 139935007315776 builder_impl.py:804] Assets written to: /tmp/exported/saved_model/assets\u001b[0m\n",
      "\u001b[34mI1213 16:05:38.527065 139935007315776 fingerprinting_utils.py:48] Writing fingerprint to /tmp/exported/saved_model/fingerprint.pb\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Writing pipeline config file to /tmp/exported/pipeline.config\u001b[0m\n",
      "\u001b[34mI1213 16:05:39.528659 139935007315776 config_util.py:253] Writing pipeline config file to /tmp/exported/pipeline.config\u001b[0m\n",
      "\u001b[34m2023-12-13 16:05:41,482 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-12-13 16:06:49 Uploading - Uploading generated training model\n",
      "2023-12-13 16:07:00 Completed - Training job completed\n",
      "Training seconds: 1970\n",
      "Billable seconds: 1970\n"
     ]
    }
   ],
   "source": [
    "tensorboard_output_config = sagemaker.debugger.TensorBoardOutputConfig(\n",
    "    s3_output_path=tensorboard_s3_prefix,\n",
    "    container_local_output_path='/opt/training/'\n",
    ")\n",
    "\n",
    "estimator = CustomFramework(\n",
    "    role=role,\n",
    "    image_uri=container,\n",
    "    entry_point='run_training.sh',\n",
    "    source_dir='source_dir/',\n",
    "    hyperparameters={\n",
    "        \"model_dir\": \"/opt/training\",        \n",
    "        \"pipeline_config_path\": \"Faster_R_CNN_ResNet152_pipeline.config\",\n",
    "        \"num_train_steps\": \"1000\",    \n",
    "        \"sample_1_of_n_eval_examples\": \"1\"\n",
    "    },\n",
    "    instance_count=1,\n",
    "    instance_type='ml.g5.2xlarge',\n",
    "    tensorboard_output_config=tensorboard_output_config,\n",
    "    disable_profiler=True,\n",
    "    base_job_name='faster-rcnn152-tf2-object-detection'\n",
    ")\n",
    "\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84545881",
   "metadata": {},
   "source": [
    "You should be able to see your model training in the AWS webapp as shown below:\n",
    "![ECR Example](../data/example_trainings.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9844f25",
   "metadata": {},
   "source": [
    "## Improve on the initial model\n",
    "\n",
    "Most likely, this initial experiment did not yield optimal results. However, you can make multiple changes to the `pipeline.config` file to improve this model. One obvious change consists in improving the data augmentation strategy. The [`preprocessor.proto`](https://github.com/tensorflow/models/blob/master/research/object_detection/protos/preprocessor.proto) file contains the different data augmentation method available in the Tf Object Detection API. Justify your choices of augmentations in the write-up.\n",
    "\n",
    "Keep in mind that the following are also available:\n",
    "* experiment with the optimizer: type of optimizer, learning rate, scheduler etc\n",
    "* experiment with the architecture. The Tf Object Detection API model zoo offers many architectures. Keep in mind that the pipeline.config file is unique for each architecture and you will have to edit it.\n",
    "* visualize results on the test frames using the `2_deploy_model` notebook available in this repository.\n",
    "\n",
    "In the cell below, write down all the different approaches you have experimented with, why you have chosen them and what you would have done if you had more time and resources. Justify your choices using the tensorboard visualizations (take screenshots and insert them in your write-up), the metrics on the evaluation set and the generated animation you have created with [this tool](../2_run_inference/2_deploy_model.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17284a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your write-up goes here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
